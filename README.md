# Project 4 - Roll Your Own CDN - Corey Sarazin

This project has been both fun and very educational and I feel I have learned a lot not only about CDNs themselves but also about the code behind how to set them up. In general, this class has probably been the hardest class I have taken in my time at Northeastern, but it has also been one of the most rewarding as I have found every project to be both challenging and time consuming but also engaging because of the real-world practicality of the assignments. As for this project, it was definitely the most intensive of the semester and the most interesting due to the server resources located around the world that are available to us. 

## Development Process and Design Decisions

For this project, I found the project to be daunting because of the size of it and the reliability we had to build into our CDN. I ended up finding myself implementing the project entirely in reverse, which I feel allowed me to focus on the things I knew well first like serving HTTP and then figuring out the more complex things I was less comfortable with.

### Http Server

I began by simply handling the HTTP requests that would come into the replica servers, and fetching them from the origin and simply ignoring the cache managment aspect of the assignment. Once I implemented that, I had some thoughts as to how to go about caching the HTTP data as effectively as possible. I knew I wanted to go for a database approach when I knew the data had to be persistent in the case any replicas had to be restarted manually. Additionally, I took the approach of pre-caching as much data as possible based on the zipf distribution that was provided. I did this through the `preLoadCache` script I made that simply takes the first n entries in the popularity list and went with a trial-and-error approach to see how many files I could store before coming close to the replica server 20MB quota. I found there could be about 220 records stored before reaching the limit, so I decided to cache about 19.5MB of data, 215 HTTP records, to allow for a little buffer to ensure there would be no issues with reaching the maximum disk usage while caching as much as possible to get the average response times as low as possible. Initially, I had the database be loaded in the deployment step, and that made the deployment take about 7 minutes. I then realized I could simply pre-load the database in my own machine and simply `scp` the database file to each replica, as the data was the same and requesting each HTTP record put a large load on the server from many replicas rather than simply from one instance. This took the deployment time down to about a minute, but I added a sleep of 3 minutes to the end of the deployment in attempts to get the autograder working but that has yielded no help so I will probably take that out for the final submission. Lastly, I also encountered an instance were stopping the CDN and immediately trying to restart it would cause a "port in use" error when trying to set up the socket, most likely due to some server-side process to garbage collect any existing port mapping data. I fixed this by adding a one minute sleep to the stop CDN process, and it seems the garbage collecting finishes in that amount of time and I have had no issues with that since. As for testing efficiency, I have been using the beacon heavily to check the consistency of my replica servers as well as the latency. I have found that my caching strategy is reliable and as efficient as possible, as I also compress the data before caching so more records can be saved. I seem to fall consistently in the upper third of the times when calculating averages by hand, only being off from the leaders by hundredths of seconds. I am overall satisfied with my approach and implementation of the HTTP server. 

### DNS Server

The DNS server is where I believe I struggled the most simply with the fact of parsing the incoming packets correctly and formatting the response such that the dig command functions properly and does not reject it for out of place flags or invalid compression values (I believe that was the term). I attempted to use the `dnslib` package in python but I was unsuccessful in getting it to format correctly and align with the necessary types for each of the objects. Therefore, I simply reverted to using the in-class slide that detailed a A type DNS request and manually parsed it, as that allowed me to keep track of the types through my own, home-rolled code. Once I got the requests to be answered with a default server, I incorporated the code to geo-locate clients based on their IP address and calculate the distance to the replica servers and return the minimum distance replica. I tried to get active connection management to work with scamper but after meeting with a TA and getting advice to just go with the IP distance approach as it is mostly similar to the active management. Combining the two pieces and adding the IP client mapping cache was relatively seamless. Again, I used the beacon page and compared my times to other peoples' times and found my times fell in the same bracket as my HTTP server. I am also satisfied with this implementation and I have not experienced any timeouts on the beacon page for my DNS server.

### deploy/run/stopCDN Scripts

Developing these scripts was a little frustrating as there were slight code bugs that I could not figure out why `scp` or `ssh` commands were erroring from my python code. I started to develop a simple shell script instead half way through because I could not figure out the deployemnt without it erroring, but I figured out the python bugs as I was half way through the shell script development. As mentioned before, I flip flopped with how specifically to deploy the resources and what commands to run server-side versus deployer-side. I feel the current setup deploys everything both properly and quickly. I also took advantage of making `scp` commands to multiple servers simultaneously to shorten deployemnt time but also saving the PIDs so that the script does not terminate until all setup commands are completed. Additionally, I added print statements to aid the deployer in seeing the progress and status of the deployment to ensure either proper error reporting or consistent status updates. 

## If I had more time ...

One of the main pieces I would have implemented if I had more time would be to create a process to run on the DNS server to check that status of the HTTP replicas. I understand the vulnerability of having the DNS server map to http replicas without knowing their status. For this inital, small-scale implementation I figured it is somewhat reasonable to monitor this personally, but in a public production release where I had a bit more time I would definitely take the time to build in reliability to have the DNS server probe HTTP replicas for their status and restart any replicas that may have crashed, and vice-versa for monitoring the DNS server from the replica side. Additionally, I would try to use this probing strategy to use dynamic IP address mapping in addition to the geo-location. Furthermore, I would also create a script that runs on some interval to update the cache database in each replica with updated content and update endpoint popularities. Also, I would try to figure out some way to consistently come as close to the disk limit by dynamically deleting certain database entries if there is a larger amount of space the replica script is taking up. 

## Testing and Autograder

I understand the autograder has been a big stress point for the staff and I understand the difficulties of setting that up, so I did not pay much attention to the outputs of it. I have not gotten my code to function with the autograder but I have been able to test it locally thoroughly and I have never seen any issues on the beacon. I went to office hours with another TA and asked if my code was failing because the stress test pushed it to error or caching limits were exceeded but they said they do not believe my coding approach does not have any vulnerabilities that would cause those errors. Therefore, I focused on the beacon and maintaining availability and consistency in those feedback areas. 

## Conclusions

Overall, I feel I have made a solid product that provides the reliability snd functionality required by a CDN. I feel I have gained the knowledge of how CDNs function and how they can be made to be efficient and resillient, and I am confident that I could take these skills and transfer them to any future work with Networking I may encounter.

